# Testing and Validation Rules

## Core Philosophy

### â›” DO NOT CREATE
- Automated test scripts (.sh, .py, etc.)
- Test harness files
- Mock/stub implementations
- CI/CD test pipelines (yet)
- Automated validation scripts

### âœ… DO CREATE
- Clear manual testing instructions
- Example curl commands in documentation
- Usage examples in code comments
- Self-debugging tools (like `unity_get_logs`)

## Why No Automated Tests (For Now)

1. **Early MVP Stage**: Focus on functionality first
2. **User Prefers Manual**: User explicitly wants to test manually
3. **Real Environment**: Testing in actual Unity environment is more valuable
4. **Fast Iteration**: No test maintenance overhead during rapid development
5. **AI-Assisted**: Claude Desktop itself is the test framework

## Manual Testing Approach

### When Adding New Feature

1. **Implement the feature**
2. **Document how to test** (in commit message or docs)
3. **Let user test** with curl or Claude Desktop
4. **Iterate based on feedback**

### Testing Documentation Format

Instead of test script, provide:

```markdown
## Testing `unity_your_tool`

### Quick Test
Open Unity, ensure server is running, then:
```bash
curl -X POST http://localhost:8765 \
  -H "Content-Type: application/json" \
  -d '{"tool": "unity_your_tool", "args": {"param": "value"}}'
```

### Expected Result
- Unity console shows: `[MCP] âœ… Tool succeeded`
- Response JSON contains: `{"success": true, ...}`
- Scene shows: [describe visible outcome]

### Test Cases to Cover
1. Default parameters: `{}`
2. Custom parameters: `{"param": "custom"}`
3. Invalid input: `{"param": null}`
4. Edge case: `{"param": "edge_case_value"}`
```

## Self-Debugging Tools

### Implementation Priority

Tools that help AI debug itself are HIGH PRIORITY:

#### Essential Self-Debugging Tools

1. **`unity_get_logs`** âœ… Implemented
   - AI can read Unity console
   - See errors/warnings/messages
   - Diagnose what went wrong

2. **`unity_capture_screenshot`** âœ… Implemented
   - AI can see visual results
   - Verify UI/objects look correct
   - Detect visual issues

3. **`unity_is_compiling`** âœ… Implemented
   - Check if Unity is busy
   - Know when to wait
   - Avoid timing issues

#### Future Self-Debugging Tools

4. **`unity_validate_scene`** (Future)
   - Check scene for common issues
   - Verify objects exist
   - Detect missing components

5. **`unity_get_performance_stats`** (Future)
   - FPS, memory usage
   - Detect performance problems
   - Optimize bottlenecks

## Testing Through Claude Desktop

### Natural Language Testing

Instead of automated tests, Claude acts as tester:

```
User: "Create a red cube and take a screenshot"

Claude: 
1. unity_create_cube({color: red})
2. unity_capture_screenshot()
3. [Analyzes image] "Created successfully, cube is red"
```

This IS the test suite.

### Benefits
- Tests real-world usage
- Catches UX issues
- Verifies AI can actually use tools
- No test maintenance

## Validation Approaches

### 1. Visual Validation
```
User manually checks:
- Unity console (no errors?)
- Unity scene (objects created?)
- Unity windows (UI works?)
```

### 2. Response Validation
```
User checks curl output:
- JSON is valid?
- Success: true?
- Data looks correct?
```

### 3. Integration Validation
```
User tests via Claude Desktop:
- Tools show up in Claude?
- Claude can use them?
- Results match expectations?
```

### 4. Self-Validation
```
AI uses its own tools:
- unity_get_logs after operations
- unity_capture_screenshot for visual
- unity_get_scene_info for state
```

## When to Add Automated Tests (Future)

Consider automated tests when:

1. **Stable API**: Tool signatures stop changing
2. **Regression Prevention**: Need to prevent breakage
3. **Complex Logic**: Business logic becomes complex
4. **Team Growth**: Multiple contributors
5. **Production Ready**: Moving beyond MVP

### Future Test Strategy

When we do add tests:

```csharp
// Unity Test Framework
[Test]
public void UnityCreateCube_WithDefaults_CreatesNamedCube()
{
    var response = MCPTools.Execute("unity_create_cube", new JObject());
    Assert.IsTrue(response["success"].ToObject<bool>());
    Assert.IsNotNull(GameObject.Find("Cube"));
}
```

```python
# Python integration tests
def test_unity_ping():
    response = requests.post("http://localhost:8765", json={
        "tool": "unity_ping",
        "args": {}
    })
    assert response.json()["success"] == True
```

But NOT YET. Focus on building functionality.

## Documentation Instead of Tests

### Good Documentation >>>> Automated Tests (for now)

**What Good Documentation Includes:**

1. **What**: Clear description of what tool does
2. **Why**: When to use this tool
3. **How**: Parameter examples
4. **Expected**: What successful result looks like
5. **Errors**: Common failure cases and solutions

Example:
```markdown
## `unity_create_cube`

Creates a primitive cube GameObject in the scene.

**When to use:** Need a basic cube for prototyping, testing, or placeholder objects.

**Parameters:**
- `name` (string, default: "Cube"): Name of the GameObject
- `position` (array, default: [0,0,0]): World position [x,y,z]
- `color` (array, optional): RGB color [r,g,b,a] (0-1 range)

**Example:**
```bash
curl -X POST http://localhost:8765 -H "Content-Type: application/json" \
  -d '{"tool": "unity_create_cube", "args": {"name": "TestCube", "position": [1,2,3]}}'
```

**Success Response:**
```json
{
  "success": true,
  "name": "TestCube",
  "instanceId": 12345,
  "position": [1,2,3]
}
```

**Common Errors:**
- "Scene is not loaded": Open a scene in Unity first
- "Invalid position": Check position array has 3 numbers
```

## Error Recovery Over Prevention

Focus on:
- **Good error messages** that explain what went wrong
- **Recovery suggestions** in error responses
- **Safe defaults** that usually work
- **Validation** that catches issues early
- **Logging** that helps debug issues

Rather than:
- Comprehensive test coverage
- All edge cases handled
- Complex validation logic
- Test fixtures and mocks

## The Real Test

**The real test is: Can an AI agent accomplish tasks using these tools?**

If Claude Desktop can:
1. âœ… Use tools to create objects
2. âœ… Verify results with screenshots
3. âœ… Read logs to debug
4. âœ… Iterate to fix issues
5. âœ… Complete user's request

Then the tools work. That's the test.

## Quality Metrics

Instead of "% test coverage", measure:

1. **Tool Success Rate**: Do tools usually return success?
2. **Error Clarity**: Can user understand error messages?
3. **AI Effectiveness**: Can AI complete tasks?
4. **Iteration Speed**: How fast can AI try â†’ verify â†’ fix?
5. **User Satisfaction**: Does user accomplish their goals?

## Summary

- â›” No automated test scripts
- âœ… Clear manual testing instructions
- âœ… Self-debugging tools for AI
- âœ… Comprehensive documentation
- âœ… Real-world usage via Claude Desktop
- âœ… Fast iteration and feedback
- ðŸ”® Automated tests later when stable
